{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use titanic data\n",
    "TRAIN_DATA_URL = \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\"\n",
    "TEST_DATA_URL = \"https://storage.googleapis.com/tf-datasets/titanic/eval.csv\"\n",
    "\n",
    "train_file_path = tf.keras.utils.get_file(\"train.csv\", TRAIN_DATA_URL)\n",
    "test_file_path = tf.keras.utils.get_file(\"eval.csv\", TEST_DATA_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data into datasets, before then, we need the info on titanic datasets\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(train_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get column info\n",
    "columns = list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>n_siblings_spouses</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>class</th>\n",
       "      <th>deck</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>alone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>Third</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>First</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>Third</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>First</td>\n",
       "      <td>C</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>male</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>Third</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Queenstown</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived     sex   age  n_siblings_spouses  parch     fare  class     deck  \\\n",
       "0         0    male  22.0                   1      0   7.2500  Third  unknown   \n",
       "1         1  female  38.0                   1      0  71.2833  First        C   \n",
       "2         1  female  26.0                   0      0   7.9250  Third  unknown   \n",
       "3         1  female  35.0                   1      0  53.1000  First        C   \n",
       "4         0    male  28.0                   0      0   8.4583  Third  unknown   \n",
       "\n",
       "   embark_town alone  \n",
       "0  Southampton     n  \n",
       "1    Cherbourg     n  \n",
       "2  Southampton     y  \n",
       "3  Southampton     n  \n",
       "4   Queenstown     y  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we can load data directly into tensorflow datasets\n",
    "ds_train = tf.data.experimental.make_csv_dataset(train_file_path, batch_size = 5, \n",
    "                                                 #We would set defaults for NULLs in columns\n",
    "                                                 select_columns = columns,\n",
    "                                                 column_defaults = [np.nan, 'NA', np.nan, np.nan, np.nan, np.nan, 'NA', 'NA', 'NA', 'NA'],\n",
    "                                                label_name= 'survived',\n",
    "                                                 na_value = 'unknown',\n",
    "                                                 num_epochs = 1,\n",
    "                                                 shuffle = False\n",
    "                                                )\n",
    "#Now we can load data directly into tensorflow datasets\n",
    "ds_test = tf.data.experimental.make_csv_dataset(test_file_path, batch_size = 5, \n",
    "                                                 #We would set defaults for NULLs in columns\n",
    "                                                 select_columns = columns,\n",
    "                                                 column_defaults = [np.nan, 'NA', np.nan, np.nan, np.nan, np.nan, 'NA', 'NA', 'NA', 'NA'],\n",
    "                                                label_name= 'survived',\n",
    "                                                 na_value = 'unknown',\n",
    "                                                 num_epochs = 1,\n",
    "                                                shuffle = False\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We next would like to count how many NULL values are there for each column in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = list(df.describe().columns)\n",
    "categorical_cols = [x for x in df.columns if x not in numeric_cols ]\n",
    "\n",
    "#numeric_cols should excluding target label\n",
    "numeric_cols.remove('survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149.0\n",
      "3.0\n",
      "0.0\n",
      "148.0166\n"
     ]
    }
   ],
   "source": [
    "#for counting NULL values, let's start from one batch and try the concept\n",
    "for i in ds_train.take(1):\n",
    "    for col in numeric_cols:\n",
    "        print(i[0][col].numpy().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#So we would define two functions of calculating # of NULL values for numerical and categorical columns\n",
    "def numeric_nulls(track, batch):\n",
    "    for col in numeric_cols:\n",
    "        track[col] += tf.cast(tf.math.count_nonzero(tf.math.is_nan(batch[0][col])), tf.int32)\n",
    "    return track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'age': <tf.Tensor: shape=(), dtype=int32, numpy=0>,\n",
       " 'n_siblings_spouses': <tf.Tensor: shape=(), dtype=int32, numpy=0>,\n",
       " 'parch': <tf.Tensor: shape=(), dtype=int32, numpy=0>,\n",
       " 'fare': <tf.Tensor: shape=(), dtype=int32, numpy=0>}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train.reduce(dict.fromkeys(numeric_cols, tf.constant(0)), numeric_nulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 627 entries, 0 to 626\n",
      "Data columns (total 10 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   survived            627 non-null    int64  \n",
      " 1   sex                 627 non-null    object \n",
      " 2   age                 627 non-null    float64\n",
      " 3   n_siblings_spouses  627 non-null    int64  \n",
      " 4   parch               627 non-null    int64  \n",
      " 5   fare                627 non-null    float64\n",
      " 6   class               627 non-null    object \n",
      " 7   deck                627 non-null    object \n",
      " 8   embark_town         627 non-null    object \n",
      " 9   alone               627 non-null    object \n",
      "dtypes: float64(2), int64(3), object(5)\n",
      "memory usage: 49.1+ KB\n"
     ]
    }
   ],
   "source": [
    "#Let's double-check if our calculation is correct\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(test_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 264 entries, 0 to 263\n",
      "Data columns (total 10 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   survived            264 non-null    int64  \n",
      " 1   sex                 264 non-null    object \n",
      " 2   age                 264 non-null    float64\n",
      " 3   n_siblings_spouses  264 non-null    int64  \n",
      " 4   parch               264 non-null    int64  \n",
      " 5   fare                264 non-null    float64\n",
      " 6   class               264 non-null    object \n",
      " 7   deck                264 non-null    object \n",
      " 8   embark_town         264 non-null    object \n",
      " 9   alone               264 non-null    object \n",
      "dtypes: float64(2), int64(3), object(5)\n",
      "memory usage: 20.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It does not seem that we need to check for NULLs anymore...\n",
    "#Next we would create feature columns out of numeric and categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First let's combine numeric columns into one called \"numeric\"\n",
    "class PackNumericFeatures():\n",
    "    def __init__(self, names):\n",
    "        self.names = names\n",
    "        \n",
    "    def __call__(self, features, labels):\n",
    "        numeric_features = [features.pop(name) for name in self.names]\n",
    "        numeric_features = [tf.cast(feature, tf.float32) for feature in numeric_features]\n",
    "        numeric_features = tf.stack(numeric_features, -1)\n",
    "        features['numeric'] = numeric_features\n",
    "        \n",
    "        return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train_packed = ds_train.map(PackNumericFeatures(numeric_cols))\n",
    "ds_test_packed = ds_test.map(PackNumericFeatures(numeric_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex                 : [b'male' b'female' b'female' b'female' b'male']\n",
      "class               : [b'Third' b'First' b'Third' b'First' b'Third']\n",
      "deck                : [b'NA' b'C' b'NA' b'C' b'NA']\n",
      "embark_town         : [b'Southampton' b'Cherbourg' b'Southampton' b'Southampton' b'Queenstown']\n",
      "alone               : [b'n' b'n' b'y' b'n' b'y']\n",
      "numeric             : [[22.      1.      0.      7.25  ]\n",
      " [38.      1.      0.     71.2833]\n",
      " [26.      0.      0.      7.925 ]\n",
      " [35.      1.      0.     53.1   ]\n",
      " [28.      0.      0.      8.4583]]\n"
     ]
    }
   ],
   "source": [
    "for i in ds_train_packed.take(1):\n",
    "    features = i[0]\n",
    "    for key, value in features.items():\n",
    "        print(f'{key:20s}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#So let's rewrite PackNumericFeatures twice to make sure we understand what is going on under the hood\n",
    "class PackNumericFeatures(): \n",
    "    #First define a class, the reason that we would like to define a class is because we would like to pass in the list of numeric column names first\n",
    "    #So we need have __init__ method to pass in the list of numeric column names\n",
    "    def __init__(self, names):\n",
    "        self.names = names\n",
    "        \n",
    "    #Now we need to specifically define a __call__ method so that it could be called like a function in dataset mapping\n",
    "    def __call__(self,features, label):\n",
    "        #Since for dataset, the map function could pass in multiple values (instead of one) based on number of elements in a batch,\n",
    "        #in this case, each batch/tuple would contain two elements - features and label\n",
    "        #Here we only need to pack the features together\n",
    "        #First we would like to get the list of numeric feature tensors, we would use list comprehension to get a feature column values iteratively\n",
    "        #At the same time, we would use .pop to remove those numeric features from the original dict\n",
    "        numeric_features = [features.pop(name) for name in self.names]\n",
    "        #Now next, because tensorflow usually works with float32, we transform every tensor in the numeric feature list into tf.float32\n",
    "        numeric_features = [tf.cast(feature, tf.float32) for feature in numeric_features]\n",
    "        #Finally, we would like to transform/stack the list of numeric features into one along the last axis - # of examples in a batch\n",
    "        numeric_features = tf.stack(numeric_features, -1)\n",
    "        #Then we would like to make sure we assign the newly created tensor back to the original features\n",
    "        features['numeric'] = numeric_features\n",
    "        \n",
    "        #return values\n",
    "        return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now write PackNumericFeatures again to memorize it\n",
    "class PackNumericFeatures():\n",
    "    def __init__(self, numeric_cols):\n",
    "        self.numeric_cols = numeric_cols\n",
    "        \n",
    "    def __call__(self, features, label):\n",
    "        numeric_features = [features.pop(numeric_col) for numeric_col in numeric_cols]\n",
    "        numeric_features = [tf.cast(feature, tf.float32) for feature in numeric_features]\n",
    "        numeric_features = tf.stack(numeric_features, -1)\n",
    "        features['numeric'] = numeric_features\n",
    "        \n",
    "        return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next we would like to create a standardization function for a numeric tensor\n",
    "def Standardize(tensor, mean, std):\n",
    "    return (tensor - mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To use the function above, first we actually need to get the mean and std, we have to get them from the original df(train)\n",
    "info = df.describe()[numeric_cols].T\n",
    "mean = info['mean']\n",
    "std = info['std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Because we already know mean and std, we would like to crete a partial function to use these two metrics\n",
    "import functools\n",
    "normalizer = functools.partial(Standardize, mean = mean, std = std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write again\n",
    "import functools\n",
    "normalizer = functools.partial(Standardize, mean = mean, std = std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Also note the above could also be achieved by creating a class like above\n",
    "#So finally, we can create our numeric feature columns\n",
    "numeric_column = tf.feature_column.numeric_column('numeric', shape = [len(numeric_cols)], normalizer_fn = normalizer)\n",
    "numeric_columns = [numeric_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's try to explain what is going on in the code above\n",
    "#1. We would like to assign feature column, specifically, numeric column, we would use tf.feature_column.numeric_column()\n",
    "#   each call here is actually for one column\n",
    "#2. The parameters passed in are 'numeric' - which is the combined numeric column we have; 'shape = 4' because we have 4 numeric features;\n",
    "#   normalizer_fn = normalizer is the standardization normalizer we have, actually it does not have to be a normalizer but any function that can\n",
    "#   transform the raw numeric input\n",
    "#3. We assign all the above to numeric_column variable, note again that this variable only represents one column, since all feature columns need to be a list,\n",
    "#   we temporarily put up a list called numeric_columns contain only numeric_column so that later we can append numeric_columns to other feature column lists\n",
    "numeric_column = tf.feature_column.numeric_column('numeric', shape = [len(numeric_cols)], normalizer_fn =  normalizer)\n",
    "numeric_columns = [numeric_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now to create feature columns for categorical columns, we need to get the vocabulary list for each column\n",
    "categories = {\n",
    "    'sex': ['male', 'female'],\n",
    "    'class' : ['First', 'Second', 'Third'],\n",
    "    'deck' : ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'],\n",
    "    'embark_town' : ['Cherbourg', 'Southhampton', 'Queenstown'],\n",
    "    'alone' : ['y', 'n']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Even so, we need to create categorical columns one by one\n",
    "categorical_columns = [tf.feature_column.categorical_column_with_vocabulary_list(key, value) for key, value in categories.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#However, we still need to transform the categorical_columns using one-hot encoding or other encodings, default one-encoding here\n",
    "categorical_columns = [tf.feature_column.indicator_column(categorical_column) for categorical_column in categorical_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we have all feature columns, we can create the DenseFeatures layer\n",
    "feature_layer = tf.keras.layers.DenseFeatures(numeric_columns + categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we can create a NN model to test\n",
    "model = tf.keras.Sequential()\n",
    "model.add(feature_layer)\n",
    "model.add(tf.keras.layers.Dense(32, activation = 'relu')) #Note no input_shape needs to be specified\n",
    "model.add(tf.keras.layers.Dense(32, activation = 'relu'))\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "model.compile(loss = tf.keras.losses.BinaryCrossentropy(from_logits = True), metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only shuffle when necessary and need to create separate copy of the dataset because we are not able to \"unshuffle\"\n",
    "training_set = ds_train_packed.unbatch().shuffle(1000).batch(5)\n",
    "testing_set = ds_test_packed.unbatch().shuffle(1000).batch(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "126/126 [==============================] - 1s 5ms/step - loss: 0.6014 - accuracy: 0.6236 - val_loss: 0.5315 - val_accuracy: 0.7045\n",
      "Epoch 2/20\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.4760 - accuracy: 0.7751 - val_loss: 0.4788 - val_accuracy: 0.7841\n",
      "Epoch 3/20\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.4331 - accuracy: 0.8182 - val_loss: 0.4714 - val_accuracy: 0.7879\n",
      "Epoch 4/20\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.4160 - accuracy: 0.8246 - val_loss: 0.4657 - val_accuracy: 0.7992\n",
      "Epoch 5/20\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.4095 - accuracy: 0.8246 - val_loss: 0.4629 - val_accuracy: 0.7955\n",
      "Epoch 6/20\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.4001 - accuracy: 0.8373 - val_loss: 0.4589 - val_accuracy: 0.8030\n",
      "Epoch 7/20\n",
      "126/126 [==============================] - 0s 3ms/step - loss: 0.3943 - accuracy: 0.8341 - val_loss: 0.4443 - val_accuracy: 0.8030\n",
      "Epoch 8/20\n",
      "126/126 [==============================] - 0s 3ms/step - loss: 0.3860 - accuracy: 0.8389 - val_loss: 0.4375 - val_accuracy: 0.8106\n",
      "Epoch 9/20\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.3832 - accuracy: 0.8357 - val_loss: 0.4357 - val_accuracy: 0.8106\n",
      "Epoch 10/20\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.3778 - accuracy: 0.8373 - val_loss: 0.4375 - val_accuracy: 0.8106\n",
      "Epoch 11/20\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.3733 - accuracy: 0.8405 - val_loss: 0.4329 - val_accuracy: 0.8220\n",
      "Epoch 12/20\n",
      "126/126 [==============================] - 0s 4ms/step - loss: 0.3693 - accuracy: 0.8437 - val_loss: 0.4289 - val_accuracy: 0.8333\n",
      "Epoch 13/20\n",
      "126/126 [==============================] - 0s 3ms/step - loss: 0.3672 - accuracy: 0.8373 - val_loss: 0.4346 - val_accuracy: 0.8258\n",
      "Epoch 14/20\n",
      "126/126 [==============================] - 0s 3ms/step - loss: 0.3667 - accuracy: 0.8453 - val_loss: 0.4312 - val_accuracy: 0.8371\n",
      "Epoch 15/20\n",
      "126/126 [==============================] - 0s 3ms/step - loss: 0.3621 - accuracy: 0.8389 - val_loss: 0.4267 - val_accuracy: 0.8295\n",
      "Epoch 16/20\n",
      "126/126 [==============================] - 0s 3ms/step - loss: 0.3570 - accuracy: 0.8389 - val_loss: 0.4338 - val_accuracy: 0.8409\n",
      "Epoch 17/20\n",
      "126/126 [==============================] - 0s 3ms/step - loss: 0.3583 - accuracy: 0.8453 - val_loss: 0.4272 - val_accuracy: 0.8409\n",
      "Epoch 18/20\n",
      "126/126 [==============================] - 0s 3ms/step - loss: 0.3558 - accuracy: 0.8421 - val_loss: 0.4329 - val_accuracy: 0.8409\n",
      "Epoch 19/20\n",
      "126/126 [==============================] - 0s 3ms/step - loss: 0.3549 - accuracy: 0.8485 - val_loss: 0.4230 - val_accuracy: 0.8447\n",
      "Epoch 20/20\n",
      "126/126 [==============================] - 0s 3ms/step - loss: 0.3530 - accuracy: 0.8389 - val_loss: 0.4221 - val_accuracy: 0.8447\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x23c26dd1448>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we can fit and evaluate the model\n",
    "model.fit(training_set, epochs = 20, validation_data = testing_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 2ms/step - loss: 0.4221 - accuracy: 0.8447\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.42213958501815796, 0.8446969985961914]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(testing_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Finally let's compare the predicted outcomes with actual outcomes\n",
    "#Let's only take two batches, note here we do not use the shuffle dataset here because it would produce different results each time\n",
    "test = ds_test_packed.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_prob = sigmoid(model.predict(test)).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "actual_outcome = []\n",
    "for batch in test:\n",
    "    actual_outcome = np.append(actual_outcome, batch[1].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The actual outcome is: Not Survived         and the predicted survival probability is 12.24%.\n",
      "The actual outcome is: Not Survived         and the predicted survival probability is 28.60%.\n",
      "The actual outcome is: Survived             and the predicted survival probability is 83.66%.\n",
      "The actual outcome is: Survived             and the predicted survival probability is 85.80%.\n",
      "The actual outcome is: Survived             and the predicted survival probability is 7.77%.\n",
      "The actual outcome is: Survived             and the predicted survival probability is 89.13%.\n",
      "The actual outcome is: Not Survived         and the predicted survival probability is 25.81%.\n",
      "The actual outcome is: Not Survived         and the predicted survival probability is 7.87%.\n",
      "The actual outcome is: Not Survived         and the predicted survival probability is 45.12%.\n",
      "The actual outcome is: Survived             and the predicted survival probability is 87.69%.\n"
     ]
    }
   ],
   "source": [
    "for ac, pr in zip(actual_outcome, pred_prob):\n",
    "    print(f'The actual outcome is: {\"Survived\" if ac == 1 else \"Not Survived\":20} and the predicted',\n",
    "             f'survival probability is {pr:.2%}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
